ENTSO-E Energy Pipeline

End-to-End Data Engineering Project

This repository contains an end-to-end data engineering pipeline for processing ENTSO-E day-ahead electricity prices across multiple European bidding zones.

The pipeline covers the full lifecycle of data:
API ingestion â†’ cloud storage â†’ warehouse modeling â†’ analytics-ready mart.


ğŸ—ï¸ Architecture Overview

ENTSO-E API
   â†“ (Python)
AWS S3 (raw JSON)
   â†“
Snowflake (RAW / VARIANT)
   â†“
dbt Staging
   â†“
Dimensions + Fact
   â†“
Analytics Mart (BI-ready)

![alt text](dbt-linear-graph-1.png)


ğŸ“‚ Repository Structure

entsoe-energy-pipeline/
â”‚
â”œâ”€â”€ ingestion/                 # Python & Lambda ingestion
â”‚   â”œâ”€â”€ entsoe_api/
â”‚   â”‚   â””â”€â”€ fetch_day_ahead_prices.py
â”‚   â””â”€â”€ lambda/
â”‚       â””â”€â”€ handler.py
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ snowflake/
â”‚       â”œâ”€â”€ roles.sql
â”‚       â”œâ”€â”€ stages.sql
â”‚       â”œâ”€â”€ pipes.sql
â”‚       â””â”€â”€ streams_tasks.sql
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ dimensions/
â”‚   â”‚   â”œâ”€â”€ facts/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ snapshots/
â”‚   â”œâ”€â”€ analyses/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ docs/                      # dbt docs (GitHub Pages)
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore


ğŸ”„ Pipeline Walkthrough

1ï¸âƒ£ Data Ingestion (Python / AWS)

Python is used to call the ENTSO-E API

Responses are stored as raw JSON files in AWS S3

Designed to be reusable for:

local runs

AWS Lambda

future orchestration (Airflow)

2ï¸âƒ£ Raw Storage (Snowflake)

Raw JSON files are ingested into Snowflake using:

external stages

VARIANT columns

Raw data is preserved unchanged for traceability and replay

3ï¸âƒ£ Staging Layer (dbt)

Model: stg_entsoe_day_ahead_prices

Responsibilities:

Parse semi-structured JSON

Extract:

bidding zone

delivery date

hourly position (1â€“24)

price (EUR/MWh)

Preserve lineage:

filename

load_time

4ï¸âƒ£ Dimensions
dim_bidding_zone

Maps bidding zone codes (e.g. DE-LU, SE4) to human-readable names

Improves usability for non-technical stakeholders

Enables geographic and map-based analysis in BI tools

dim_date

Standard date dimension

Includes:

year, quarter, month

weekday / weekend flags

5ï¸âƒ£ Fact Table
fct_day_ahead_price

Grain: one record per (zone, delivery_date, hour position)

Incremental model

Derives:

delivery_datetime

period_of_day (Early Hours, Morning, Midday, Evening, Night)

Preserves all published records

âš ï¸ ENTSO-E may publish multiple prices for the same hour due to corrections.
These are intentionally preserved to reflect real market behavior.

6ï¸âƒ£ Analytics Mart (BI-Ready)
mart_day_ahead_prices

View optimized for analytics consumption

Joins:

fact + bidding zone + date dimension

Exposes:

readable zone names

calendar attributes

intraday buckets

price measures

ğŸ§ª Data Quality & Testing

dbt tests are applied at the mart level:

not_null tests on all business-critical columns

Uniqueness is intentionally not enforced

Reasoning:
Market data can contain legitimate duplicates due to re-publishing.
In a production setup, versioning or â€œlatest-priceâ€ logic would be added.

ğŸ“˜ Documentation & Lineage

dbt documentation is generated using:

"dbt docs generate"

The documentation includes:

-column-level descriptions

-model dependencies

-full lineage graph

ğŸ› ï¸ Tech Stack

Python â€“ API ingestion and data extraction

AWS S3 â€“ Raw data storage

Snowflake â€“ Cloud data warehouse

dbt Core â€“ Transformations, testing, and documentation

SQL â€“ Data modeling

Git / GitHub â€“ Version control and project sharing

ğŸš€ Future Enhancements

Price versioning & late-arriving corrections

Snapshot-based historical tracking

Airflow orchestration

Power BI dashboards built on the mart

ğŸ“Œ Project Summary

ENTSO-E Energy Pipeline provides daily electricity prices across multiple European countries, modeled using modern data engineering best practices and designed for scalable analytics consumption.